{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1 - Carrera de Robots\n",
    "\n",
    "_Julia García Martínez y Robert Esclapez García_\n",
    "\n",
    "\n",
    "\n",
    "### 1. Introducción\n",
    "\n",
    "En nuestra práctica hemos realizado los apartados de `1.1.Navegación`, `1.2.Imagen Stereo` y `1.3.Detección de otro robot`.\n",
    "  \n",
    "Tanto para la navegación como para la detección del segundo robot hemos usado la solución basada en _DeepLearning_. La red usada en ambos casos se trata de [MobileNet](https://keras.io/applications/#mobilenet). _MobileNet_ está pensada para aplicaciones de visión móvil y se basa en **depthwise separable convolutions**, lo que la hace eficiente.\n",
    "  \n",
    "En el apartado de `Aproximación` explicaremos los motivos por los cuales la hemos elegido.\n",
    "  \n",
    "En el apartado de _navegación_ teníamos como reto conseguir que el robot realizara el circuito que se nos da inicialmente. Las partes que más problemas **pensábamos** que podrían darnos eran las siguientes:\n",
    "  \n",
    "  ![alt text](images/zigzag.png \"Zig zag\")\n",
    "  ![alt text](images/rotonda.png \"Rotonda\")\n",
    "  ![alt text](images/callejon.png \"Callejón sin salida\")\n",
    "  \n",
    "Sin embargo, en los siguientes apartados veremos que esto no era del todo cierto.\n",
    " \n",
    "  \n",
    "En cuanto a la _imagen stereo_ se nos pedía obtener la **nube de puntos en 3D** a partir de las cámaras traseras del robot. En un **principio**, lo hicimos sacando los puntos característicos de las imágenes. Una vez que los obtuvimos hicimos un filtrado para eleminar los puntos que no aportaban gran información (_código del algoritmo abajo_). A continuación, usando [match](https://docs.opencv.org/2.4/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.html) identificábamos coincidencias entre las dos imágenes y obteníamos un resultado rectificado. Sin embargo, no obteníamos el resultado requerido.\n",
    "  \n",
    "\n",
    "```cpp\n",
    "    // min distance\n",
    "    double min_dist = DBL_MAX;\n",
    "\n",
    "    for (int i = 0; i < (int)matches.size(); i++)\n",
    "    { \n",
    "        double dist = matches[i].distance;\n",
    "        if (dist < min_dist) \n",
    "            min_dist = dist;\n",
    "    }\n",
    "\n",
    "    if (min_dist < 1.0) \n",
    "        min_dist = 1.0;\n",
    "\n",
    "    // save good matches\n",
    "    const double threshold = 1.5 * min_dist;\n",
    "\n",
    "    std::vector<cv::DMatch> matches_good;\n",
    "\n",
    "    for (int i = 0; i < (int)matches.size(); i++) \n",
    "    {\n",
    "        if (matches[i].distance < threshold) \n",
    "        {\n",
    "            matches_good.push_back(matches[i]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "  \n",
    "Por último, para la _detección del robot_ debíamos entrenar nuestra red para que reconociera a **robot2**. Como ya teníamos la red del apartado de _navegación_, nos centramos en conseguir un buen `dataset`. Básicamente obtuvimos muchas imágenes del robot en distintas posiciones, a distintas distancias con respecto el **robot1** y poniendo como obstáculos las paredes del circuito.\n",
    "  \n",
    "  \n",
    "\n",
    "### 2. Aproximación\n",
    "  \n",
    "#### 1.1.Navegación\n",
    "  \n",
    "> Este apartado al tratarse del eje central de la práctica ha sido el que más tiempo nos ha conllevado y en el que más nos hemos tropezado. El crear un buen capturador de datos, realizar un dataset completo para que la red entrenara adecuadamente, elegir correctamente el tipo de red a usar, etc. eran fundamentales para que el robot completara el circuito.\n",
    "  \n",
    "En nuestro proyecto final (_en `Experimentación` explicaremos las distintas versiones usadas_) para el dataset recogemos las imágenes de la cámara del robot y si va a **izquierda** o **derecha**.\n",
    "Para la guardar las imágenes hemos usado `cv_bridge` tanto en la navegación como en la detección del otro robot:\n",
    "  \n",
    "\n",
    "```cpp\n",
    "    cv::Mat dest;\n",
    "    cv::resize(cv_bridge::toCvShare(imageMsg, \"bgr8\")->image, dest, cv::Size(imageSize, imageSize));\n",
    "    cv::imshow(\"Vista Conductor\", dest);\n",
    "    cv::imwrite(fileName, dest);\n",
    "    cv::waitKey(30);\n",
    "```\n",
    "\n",
    "  \n",
    "> Además, para hacer más fácil la obtención de datos, cambiamos la manera de mover el robot por teclado usando `w`, `a` y `d` sin necesidad de pulsar `Enter`.\n",
    "  \n",
    "  \n",
    "  \n",
    "Para poder entrenar correctamente las distintas redes que queríamos probar sin tener que preocuparnos por la limitación de nuestros ordenadores usamos [Google Colaboratory](https://research.google.com/colaboratory/faq.html). Colaboratory es una herramienta de investigación para aprender y explorar el aprendizaje automático. Gracias a su entorno de ejecución con GPU, pudimos entrenar las redes rápidamente.\n",
    "  \n",
    "Este es el modelo de red que hemos usado en la versión final:\n",
    "\n",
    "\n",
    "```python\n",
    "       def cnn_model(input_shape):\n",
    "            mobilenet_model = MobileNet(input_shape=input_shape,\n",
    "                                        input_tensor=input_tensor,\n",
    "                                        pooling='avg',\n",
    "                                        include_top=False,\n",
    "                                        weights='imagenet')\n",
    "            x = mobilenet_model.output\n",
    "            prediction = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "            model = Model(inputs=input_tensor, outputs=prediction)\n",
    "\n",
    "            return model\n",
    "\n",
    "```\n",
    "\n",
    "  \n",
    "Como se puede observar, el modelo es preentrenado inicialmente con `ImageNet` y usa la función `sigmoid` que nos devuelve valores entre **0** y **1** (_izquierda_ o _derecha_).\n",
    "\n",
    "\n",
    "  ```python  \n",
    "           model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "           filepath = './results/weights.{epoch:02d}-{loss:.6f}.hdf5'\n",
    "           model_checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, mode='auto', period=1)\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "Para compilar el modelo era muy importante usar como función de _loss_ `mse`, puesto que el objetivo de la red era minimizar el _loss_ lo máximo posible. A continuación, definíamos con **ModelCheckpoint** la manera en que queríamos guardar los pesos. En nuestro caso, en cada época generábamos un `.hdf5`.\n",
    "  \n",
    "La red ha sido entrenada con 100 épocas, 64 de batch y tardaba **78s** por época (aprox.). En total eran 2 horas (aprox.).\n",
    "  \n",
    "  \n",
    "Por último, para la navegación del robot:\n",
    "  1. Nos subscribíamos a la cámara para ir obteniendo en tiempo real lo que visualizaba el robot.\n",
    "  2. Con el mejor peso obtenido por la red entrenábamos el modelo.\n",
    "  3. El modelo iba devolviendo la acción que debía realizar el robot.\n",
    "  4. Publicábamos al nodo **'/robot1/mobile_base/commands/velocity'** la velocidad linear y angular obtenida como salida de la red.\n",
    "  \n",
    "\n",
    "```python\n",
    "       if (prediction[0][0]*100) < 50:\n",
    "        move_cmd.angular.z += 0.4\n",
    "        move_cmd.linear.x += 0.3\n",
    "       else:\n",
    "        move_cmd.angular.z += -0.4\n",
    "        move_cmd.linear.x += 0.3\n",
    "\n",
    "        self.cmd_vel.publish(move_cmd)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "  #### 1.2.Imagen Stereo\n",
    "  \n",
    "Para la realización de la _imagen stereo_ vamos a necesitar obtener dos imágenes no muy distantes entre ellas. Por lo que nos vamos a subscribir a las dos cámaras traseras de `TurtleBot`. Las imágenes deben ser del mismo tamaño (`altura`**x**`anchura`) y la misma cantidad de canales.\n",
    "  \n",
    "Una vez obtenemos las dos imágenes del robot aplicamos el algoritmo de _Heiko Hirschmuller_ implementado en la clase `SGBM` de la librería `openCV`. Dados unos parámetros y las dos imágenes de entrada, obtenemos la imagen de dispersión. \n",
    "  \n",
    "Con la imagen de dispersión y la matriz `Q` generamos la nube de puntos que estábamos buscando.\n",
    "  \n",
    ">Ejemplo de salida.\n",
    "  \n",
    "\n",
    "  ![alt text](images/pared0.png \"Foto original\")\n",
    "  ![alt text](images/pared1.png \"Salida\")\n",
    "  ![alt text](images/pared2.png \"Salida\")\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#### 1.3.Detección de otro robot\n",
    "  \n",
    ">Este último apartado era muy parecido al de la navegación. La diferencia recaía en el tipo de dataset generado y en el tipo de salida que devolvía el algoritmo.\n",
    "  \n",
    "La red ha sido entrenada con 300 épocas, 64 de batch y tardaba **10s** por época (aprox.). En total eran 50 mins (aprox.).\n",
    "  \n",
    "Como se ha mencionado anteriormente, hemos usado `cv_bridge` para la captura de imágenes. Algunos ejemplos de las muestras tomadas del **robot2**:\n",
    "  \n",
    "  ![alt text](images/example1.png \"Ejemplo 1\")\n",
    "  ![alt text](images/example2.png \"Ejemplo 2\")\n",
    "  ![alt text](images/example3.png \"Ejemplo 3\")\n",
    "  ![alt text](images/example4.png \"Ejemplo 4\")\n",
    "  ![alt text](images/example5.png \"Ejemplo 5\")\n",
    "  \n",
    "  \n",
    "Como hemos mencionado en la introducción, la red usada para este apartado es la misma que para la _navegación_. La única diferencia es que no ha sido necesario preentrenar la red con `ImageNet`.\n",
    "  \n",
    "La salida obtenida es el porcentaje de detección del otro robot.\n",
    "  \n",
    "  \n",
    "\n",
    "### 3. Experimentación\n",
    "\n",
    "#### 1.1.Navegación\n",
    "  \n",
    "Lo primero que empezamos a realizar fue el conjunto de datos. Al principio capturábamos el **incremento** que se añadían a las velocidades linear y angular y las imágenes **cada vez que pulsábamos una tecla**. El problema que esto conllevaba era la poca cantidad de información que se obtenía. Entonces, decidimos recoger en bucle dicha información.\n",
    "  \n",
    "Tras consultar una duda en clase nos dimos cuenta que a nosotros realmente no nos interesaba obtener dicho incremento, puesto que no era preciso en cuanto al momento en que se tomaba la imagen. Pasamos a tomar muestras de la **Odometría** del robot utilizando la librería `nav_msgs/Odometry` y `TimeSynchronizer` para definir un _timestamp_ que recogiera tanto las imágenes como dicha odometría en el mismo punto de tiempo.\n",
    "  \n",
    "Una vez que finalizamos la recogida de datos pasamos a investigar acerca del tipo de red a utilizar y **dónde** probarla, puesto que, como hemos mencionado antes, no teníamos recursos para ello.\n",
    "  \n",
    "Trasteamos el tema de cómo subir el dataset a Colaboratory y aprender a usar este servidor. Probamos una red que hicimos en la asignatura _Desafíos de programación_ que no funcionaba correctamente puesto que servía para la clasificación de distintas notas musicales.\n",
    "  \n",
    "  ![alt text](images/dp.jpg \"Red de DP\")\n",
    "  ![alt text](images/dp2.jpg \"Red de DP\")\n",
    "  \n",
    "Como se puede observar los resultados son **sospechosamente** buenos (?). Inmediatamente la descartamos.\n",
    "  \n",
    "Seguidamente nos metimos de lleno a investigar sobre las redes _ResNet_ y probamos la que proporcionaba _Keras_: `ResNet50`.\n",
    "  \n",
    "Tras configurarla de distintas maneras para intentar que se ejecutara tuvimos que desecharla. El problema estaba en que, cuando comenzaba a entrenar, se detenía el entorno antes de la primera época.\n",
    "  \n",
    "Inmediatamente buscamos otras opciones y fue cuando encontramos `MobileNet`. Empezamos a generar pesos y a guardarnos los mejores de cada ejecución. Cuando empezamos a probarlos vimos que fallaba en el zigzag, el callejón sin salida y ciertas curvas muy cerradas.\n",
    "  \n",
    "  ![alt text](images/choque.gif \"Choque del robot\")\n",
    "  \n",
    ">_Ejemplo de fallo_\n",
    "  \n",
    "Decidimos cambiar la odometría por simplemente un valor de **0** o **1** (_izquierda o derecha_). Sin embargo, seguía estando el problema del zigzag y la curva de antes del callejón. Entonces, finalmente, llegamos a la conclusión de que se debía a que el dataset debía completarse con más ejemplos de ese tipo.\n",
    "  \n",
    "La manera de afrontar esto fue con la siguiente estrategia: hacíamos que el robot serpenteara, por lo tanto, en el momento en que nos movíamos mucho a la izquierda, el robot tendría en su campo de visión mucha pared a la izquierda y cuando se acercaba mucho a ésta girábamos a la derecha. De esta manera generamos un dataset lo suficientemente preparado para afrontar todas las curvas.\n",
    "  \n",
    "  \n",
    "#### 1.2.Imagen Stereo\n",
    "  \n",
    "En este apartado hemos probado los siguientes puntos:\n",
    "  \n",
    "  + Diferentes combinaciones de parámetros hasta obtener un resultado deseable.\n",
    "\n",
    "\n",
    "```cpp\n",
    "  //Ptr<StereoSGBM> sgbm = StereoSGBM::create(-10,16,11,0,4000,32,0,15,1000,16,StereoSGBM::MODE_HH);\n",
    "  Ptr<StereoSGBM> sgbm = StereoSGBM::create(-55, 32, 11, 120, 2000, 32, 0, 15, 1000, 16,StereoSGBM::MODE_HH);\n",
    "``` \n",
    "\n",
    "\n",
    "  \n",
    "  + Utilizar _Surf_ como detector de puntos característicos de ambas de imágenes. Acto seguido, mediante el `matcher` de _FLANN_ obtener las coincidencias de las imágenes, filtrarlas por la **distancia mínima** de los puntos característicos de todos los matches multiplicada por un `threshold`, para quedarnos únicamente con los mejor matches.\n",
    "  Finalmente obtenemos una imagen en base a esos matches. Faltaría obtener la imagen de disparidad del resultado y llevarlo a la nube de puntos 3D, pero nos daba problemas y al final no se pudo realizar.\n",
    "  \n",
    "  \n",
    "  + Intento de calibración de las cámaras subscribiéndonos a los tópicos informativos del `TurtleBot` sobre las cámaras (`camera_info`). Una vez obtenidas las matrices de las cámaras (_matriz intrínseca_ y _matriz de proyección_) intentar rectificar las imágenes. Abandonada la idea por falta de documentación tanto en los **pdfs** como en la **web**.\n",
    "  \n",
    "  \n",
    "#### 1.3.Detección de otro robot\n",
    "\n",
    "La primera prueba que realizamos fue llevar a cabo unas **900** fotos aproximadamente donde el robot aparecía y no aparecía. Como utilizábamos una red con salida `sigmoid`, si el robot se encuentraba en la imagen, la salida que devolvía era un **1** y en caso contrario un **0**.\n",
    "  \n",
    "El dataset fueron 900 imágenes normalizadas, es decir, 450 con el robot y otras 450 sin él. Las fotos y la apariencia del robot fueron por todo el circuito en diferentes zonas. No se realizaron más pruebas porque obtuvimos unos resultados muy buenos con el primer modelo usado y el dataset. Mostraba la apariencia del robot con una precisión de 99% incluso cuando medio robot estaba tapado por una pared y a bastante distancia.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Conclusiones\n",
    "\n",
    "En esta práctica no sólo hemos ampliado nuestros conocimiento de _DeepLearning_, también hemos aprendido a usar el framework **ROS**.\n",
    "  \n",
    "Por otro lado, dado que ROS funcionaba correctamente en sistemas _Linux_ y nuestros portátiles con **VirtualBox** no lo soportaban, tuvimos que buscar otros medios. Para el caso del Macbook utilizamos el entorno de virtualización **Parallels Desktop**, puesto que soportaba correctamente ROS y Gazebo al extraer el máximo rendimiento del ordenador. En el caso del segundo portátil con Windows 10 simplemente hicimos una partición e instalamos _Ubuntu_.\n",
    "  \n",
    "Una de las cosas que más nos frustró fue perder tanto tiempo en aprender a manejarnos con ROS. El concepto con el que se manejaba este framework nos parecía al principio un poco abstracto. Sin embargo, cuando cogimos la suficiente soltura y conseguimos preparar las clases que capturaran la información necesaria todo avanzó más rápido.\n",
    "  \n",
    "Si no llega a ser por el uso que le dimos a Google Colaboratory no hubiésemos podido realizar la parte de _DeepLearning_, puesto que el entrenamiento conllevaba muchísimas horas. Pero gracias a dicha herramienta realizábamos los entrenamientos en menos de 2 horas.\n",
    "  \n",
    "Otra de las conclusiones que podemos extraer es la importancia descomunal de realizar un dataset de **calidad**. Prima mucho más la calidad que la cantidad. Totalmente dependiente del modelo de la red. Por ejemplo, al nosotros usar una red con salida sigmoidea (valor entre 0 y 1) no podíamos realizar las rectas pulsando continuamente \"_Izquierda-Derecha-Izquierda-Derecha-..._\" ya que para una misma imagen le estamos diciendo que puede ser derecha e izquierda al mismo tiempo. Este punto lo solventamos como hemos descrito en el apartado de `Experimentación`.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Referencias\n",
    "\n",
    "  [message_filters](http://wiki.ros.org/message_filters)\n",
    "\n",
    "  [nav_msgs/Odometry](http://docs.ros.org/api/nav_msgs/html/msg/Odometry.html)\n",
    "\n",
    "  [cv_bridge](http://wiki.ros.org/cv_bridge)\n",
    "  \n",
    "  [Converting between ROS images and OpenCV images](http://wiki.ros.org/cv_bridge/Tutorials/UsingCvBridgeToConvertBetweenROSImagesAndOpenCVImages)\n",
    "\n",
    "  [ROS: Documentation](http://wiki.ros.org)\n",
    "  \n",
    "  [ResNet50](https://keras.io/applications/#resnet50)\n",
    "\n",
    "  [MobileNet](https://keras.io/applications/#mobilenet)\n",
    "  \n",
    "  [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "  \n",
    "  [Convolutions Types](https://ikhlestov.github.io/pages/machine-learning/convolutions-types/#depthwise-separable-convolutions-separable-convolutions)\n",
    "  \n",
    "  [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
